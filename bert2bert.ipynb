{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert2bert.ipynb","provenance":[{"file_id":"https://gist.github.com/aicrowd-bot/5ee638c74004bda071bda1a58ceac44c","timestamp":1654793673220}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1bc3edd8d95942d88938a93427c389f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_437e18081ae44115865585702c8ded76","IPY_MODEL_71e49a9f5f0842ed88517572cc96c4b7","IPY_MODEL_2a6f4176a9f9474f96176103b751a4d3"],"layout":"IPY_MODEL_1d2b70a1274f46538ca3fa59d01d47a6"}},"437e18081ae44115865585702c8ded76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49fb7399780e45cc8912a039615300d5","placeholder":"​","style":"IPY_MODEL_3219109ddda04e5c82ce8cf161e97134","value":"official_ranking.csv: 100%"}},"71e49a9f5f0842ed88517572cc96c4b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46858262a376437baf237be43786ac37","max":19444,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d13c26a525243a7b70704c509d7bfe4","value":19444}},"2a6f4176a9f9474f96176103b751a4d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6f155cdda2c440f9e26a3c1c197a3d8","placeholder":"​","style":"IPY_MODEL_643b4d2563dc4360af8deb9d313c636d","value":" 19.4k/19.4k [00:00&lt;00:00, 295kB/s]"}},"1d2b70a1274f46538ca3fa59d01d47a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49fb7399780e45cc8912a039615300d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3219109ddda04e5c82ce8cf161e97134":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46858262a376437baf237be43786ac37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d13c26a525243a7b70704c509d7bfe4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6f155cdda2c440f9e26a3c1c197a3d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"643b4d2563dc4360af8deb9d313c636d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f776ac9a8897426995a020dd88444660":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_290a9fe02e084d989b0d9b77319d267d","IPY_MODEL_3031bb77c9234742add59cac373cda7d","IPY_MODEL_0da0cca44a6146579ecd982359de5aa8"],"layout":"IPY_MODEL_8c1534e80c764b18b5f0d407e5ac499e"}},"290a9fe02e084d989b0d9b77319d267d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83a765f65b7f48b8a322cb9807b1b1b1","placeholder":"​","style":"IPY_MODEL_9cdbc6664019428ab8ce68ad6d139353","value":"original_test.csv: 100%"}},"3031bb77c9234742add59cac373cda7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71abad6ad6c14b2db24e481a9da6b336","max":37223,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb86f6d784dd4682bf4788299ac23cd5","value":37223}},"0da0cca44a6146579ecd982359de5aa8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8468cc4586944ad9895b41174e44600d","placeholder":"​","style":"IPY_MODEL_36cde98946b947c9ba0066ba32c2fb1d","value":" 37.2k/37.2k [00:00&lt;00:00, 255kB/s]"}},"8c1534e80c764b18b5f0d407e5ac499e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83a765f65b7f48b8a322cb9807b1b1b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cdbc6664019428ab8ce68ad6d139353":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71abad6ad6c14b2db24e481a9da6b336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb86f6d784dd4682bf4788299ac23cd5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8468cc4586944ad9895b41174e44600d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36cde98946b947c9ba0066ba32c2fb1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d21948a489a424eb9b2ec3c3b3a46a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_574b874493774cc9b237d0a7ec9f5c11","IPY_MODEL_b77e5bf533c240a2bbfd80af835e7523","IPY_MODEL_ef2fb79981ee40b79a1f506fc60dbbe1"],"layout":"IPY_MODEL_77bb0752a4f042d6b79e7860b58e88bb"}},"574b874493774cc9b237d0a7ec9f5c11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d442927188c74cc6a22edebbc411e956","placeholder":"​","style":"IPY_MODEL_97d62cf9fc8541a1a3d23c9d395ecbbd","value":"synthetic_test.csv: 100%"}},"b77e5bf533c240a2bbfd80af835e7523":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0079ab443bba45a2990465c1d8ad9699","max":34212,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df412c328eba417f84f012ee6e7d9cc2","value":34212}},"ef2fb79981ee40b79a1f506fc60dbbe1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82e6246a5b654c8db0ca65f4f9e52f8d","placeholder":"​","style":"IPY_MODEL_ee3c461c110d42c09bb667afc2577b6d","value":" 34.2k/34.2k [00:00&lt;00:00, 252kB/s]"}},"77bb0752a4f042d6b79e7860b58e88bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d442927188c74cc6a22edebbc411e956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97d62cf9fc8541a1a3d23c9d395ecbbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0079ab443bba45a2990465c1d8ad9699":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df412c328eba417f84f012ee6e7d9cc2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82e6246a5b654c8db0ca65f4f9e52f8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee3c461c110d42c09bb667afc2577b6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4438149f97154cd2b7c293d80f619ff1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_918309ea2812474b8dc861220e8b6603","IPY_MODEL_ef2d735577d748aab3e262c5540f2fb1","IPY_MODEL_a9916e56cc014694b1ac59fcffff1097"],"layout":"IPY_MODEL_4b79250b137f4335b79eb5283ef03b56"}},"918309ea2812474b8dc861220e8b6603":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db329a54265d44b7a8434150a7b4435f","placeholder":"​","style":"IPY_MODEL_8b78fbdfea6f4f5991928a83a9f36f1a","value":"test.csv: 100%"}},"ef2d735577d748aab3e262c5540f2fb1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b7c9564565947d38574d0186238b065","max":45547,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ba4d94f57e34d13be9e75f64009d18d","value":45547}},"a9916e56cc014694b1ac59fcffff1097":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6062063bf13f42b8ba986972c398ec86","placeholder":"​","style":"IPY_MODEL_38e1759b17924e40907746fbf65c3abb","value":" 45.5k/45.5k [00:00&lt;00:00, 268kB/s]"}},"4b79250b137f4335b79eb5283ef03b56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db329a54265d44b7a8434150a7b4435f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b78fbdfea6f4f5991928a83a9f36f1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b7c9564565947d38574d0186238b065":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ba4d94f57e34d13be9e75f64009d18d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6062063bf13f42b8ba986972c398ec86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38e1759b17924e40907746fbf65c3abb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"867f1a9872454ce4aea6f0a8263ae5d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_860e14fcf87b40c7b9503555f7cc3d94","IPY_MODEL_fc5dff746b86458ca91e01ee63c6f01e","IPY_MODEL_3d0c31c1d4de42f6b7766e2e08287330"],"layout":"IPY_MODEL_60bbf6f4bc7c40428cc7bdcf7a159587"}},"860e14fcf87b40c7b9503555f7cc3d94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf0934f2569d4744b9defb5330973de5","placeholder":"​","style":"IPY_MODEL_6b8939f3a194446985818d6add2239c8","value":"train.csv: 100%"}},"fc5dff746b86458ca91e01ee63c6f01e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d56e7d218a654806a63970ca0259c2d8","max":395041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfa8c4938f6942b5ba363e01c1713e49","value":395041}},"3d0c31c1d4de42f6b7766e2e08287330":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63952a52ef14402a8b3d1d42ee9791c5","placeholder":"​","style":"IPY_MODEL_8f7f130b34464008944b3625a6dd1a25","value":" 395k/395k [00:00&lt;00:00, 654kB/s]"}},"60bbf6f4bc7c40428cc7bdcf7a159587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf0934f2569d4744b9defb5330973de5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b8939f3a194446985818d6add2239c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d56e7d218a654806a63970ca0259c2d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfa8c4938f6942b5ba363e01c1713e49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63952a52ef14402a8b3d1d42ee9791c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f7f130b34464008944b3625a6dd1a25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# LMing-Rules baseline\n","* Using language modeling.\n","* Using rules, extracted from the training data."],"metadata":{"id":"5Ajlqoq10jGm"}},{"cell_type":"markdown","source":["### Sign in\n","* To get the data."],"metadata":{"id":"nVgmJo-Y133P"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"9pIAWiB1yZv0","executionInfo":{"status":"ok","timestamp":1658773899844,"user_tz":-180,"elapsed":6970,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"outputs":[],"source":["%%capture\n","!pip install aicrowd-cli\n","%load_ext aicrowd.magic"]},{"cell_type":"code","source":["%aicrowd login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYafSj95yjEK","outputId":"fe268106-7a2f-43c8-90b6-7b0cb4a4f1c2","executionInfo":{"status":"ok","timestamp":1658773912166,"user_tz":-180,"elapsed":12340,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Please login here: \u001b[34m\u001b[1m\u001b[4mhttps://api.aicrowd.com/auth/F15gWEfVeXiTqgQycPOx4_GxnPASX0DZY263YOzlb6I\u001b[0m\n","\u001b[32mAPI Key valid\u001b[0m\n","\u001b[32mGitlab access token valid\u001b[0m\n","\u001b[32mSaved details successfully!\u001b[0m\n"]}]},{"cell_type":"code","source":["!rm -rf data\n","!mkdir data\n","%aicrowd ds dl -c htrec-2022 -o data"],"metadata":{"id":"N4ypVNCGykeI","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["1bc3edd8d95942d88938a93427c389f0","437e18081ae44115865585702c8ded76","71e49a9f5f0842ed88517572cc96c4b7","2a6f4176a9f9474f96176103b751a4d3","1d2b70a1274f46538ca3fa59d01d47a6","49fb7399780e45cc8912a039615300d5","3219109ddda04e5c82ce8cf161e97134","46858262a376437baf237be43786ac37","9d13c26a525243a7b70704c509d7bfe4","c6f155cdda2c440f9e26a3c1c197a3d8","643b4d2563dc4360af8deb9d313c636d","f776ac9a8897426995a020dd88444660","290a9fe02e084d989b0d9b77319d267d","3031bb77c9234742add59cac373cda7d","0da0cca44a6146579ecd982359de5aa8","8c1534e80c764b18b5f0d407e5ac499e","83a765f65b7f48b8a322cb9807b1b1b1","9cdbc6664019428ab8ce68ad6d139353","71abad6ad6c14b2db24e481a9da6b336","fb86f6d784dd4682bf4788299ac23cd5","8468cc4586944ad9895b41174e44600d","36cde98946b947c9ba0066ba32c2fb1d","0d21948a489a424eb9b2ec3c3b3a46a7","574b874493774cc9b237d0a7ec9f5c11","b77e5bf533c240a2bbfd80af835e7523","ef2fb79981ee40b79a1f506fc60dbbe1","77bb0752a4f042d6b79e7860b58e88bb","d442927188c74cc6a22edebbc411e956","97d62cf9fc8541a1a3d23c9d395ecbbd","0079ab443bba45a2990465c1d8ad9699","df412c328eba417f84f012ee6e7d9cc2","82e6246a5b654c8db0ca65f4f9e52f8d","ee3c461c110d42c09bb667afc2577b6d","4438149f97154cd2b7c293d80f619ff1","918309ea2812474b8dc861220e8b6603","ef2d735577d748aab3e262c5540f2fb1","a9916e56cc014694b1ac59fcffff1097","4b79250b137f4335b79eb5283ef03b56","db329a54265d44b7a8434150a7b4435f","8b78fbdfea6f4f5991928a83a9f36f1a","8b7c9564565947d38574d0186238b065","8ba4d94f57e34d13be9e75f64009d18d","6062063bf13f42b8ba986972c398ec86","38e1759b17924e40907746fbf65c3abb","867f1a9872454ce4aea6f0a8263ae5d1","860e14fcf87b40c7b9503555f7cc3d94","fc5dff746b86458ca91e01ee63c6f01e","3d0c31c1d4de42f6b7766e2e08287330","60bbf6f4bc7c40428cc7bdcf7a159587","cf0934f2569d4744b9defb5330973de5","6b8939f3a194446985818d6add2239c8","d56e7d218a654806a63970ca0259c2d8","cfa8c4938f6942b5ba363e01c1713e49","63952a52ef14402a8b3d1d42ee9791c5","8f7f130b34464008944b3625a6dd1a25"]},"executionInfo":{"status":"ok","timestamp":1658773921631,"user_tz":-180,"elapsed":9685,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"dfd66200-840e-4735-a79e-931939416c88"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["official_ranking.csv:   0%|          | 0.00/19.4k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc3edd8d95942d88938a93427c389f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["original_test.csv:   0%|          | 0.00/37.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f776ac9a8897426995a020dd88444660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["synthetic_test.csv:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d21948a489a424eb9b2ec3c3b3a46a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test.csv:   0%|          | 0.00/45.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4438149f97154cd2b7c293d80f619ff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train.csv:   0%|          | 0.00/395k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"867f1a9872454ce4aea6f0a8263ae5d1"}},"metadata":{}}]},{"cell_type":"code","source":["%%capture\n","!pip install pywer\n","import pywer\n","import pandas as pd\n","import numpy as np\n","import os"],"metadata":{"id":"xam0TODzylDO","executionInfo":{"status":"ok","timestamp":1658773929222,"user_tz":-180,"elapsed":7610,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Visualisation\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"f2RJAqufR--h","executionInfo":{"status":"ok","timestamp":1658773935709,"user_tz":-180,"elapsed":6498,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(\"data/train.csv\")\n","test_df = pd.read_csv(\"data/test.csv\")\n","print(f\"{train_df.shape[0]} train and {test_df.shape[0]} instances\"); train_df.sample()"],"metadata":{"id":"Q9pK5WLJytWD","colab":{"base_uri":"https://localhost:8080/","height":98},"executionInfo":{"status":"ok","timestamp":1658773935715,"user_tz":-180,"elapsed":46,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"8f62cb5d-07e3-4ace-9d8b-e295fa689272"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["1875 train and 338 instances\n"]},{"output_type":"execute_result","data":{"text/plain":["                        HUMAN_TRANSCRIPTION  \\\n","499  Πολύδωρον ἔκ τε πατρὸς ἐν δόμοις ἔχεις   \n","\n","                    SYSTEM_TRANSCRIPTION  CENTURY  \\\n","499  πελιδωρον. ἐκεὶ πατρὸςς ἐνδαμιτςεὐχ       16   \n","\n","                                            IMAGE_PATH  TEXT_LINE_NUM  \n","499  34 Bodleian-Library-MS-Barocci-37_00118_fol-56...              1  "],"text/html":["\n","  <div id=\"df-874d78ae-c139-4f83-a7cd-79ad484175fa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>HUMAN_TRANSCRIPTION</th>\n","      <th>SYSTEM_TRANSCRIPTION</th>\n","      <th>CENTURY</th>\n","      <th>IMAGE_PATH</th>\n","      <th>TEXT_LINE_NUM</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>499</th>\n","      <td>Πολύδωρον ἔκ τε πατρὸς ἐν δόμοις ἔχεις</td>\n","      <td>πελιδωρον. ἐκεὶ πατρὸςς ἐνδαμιτςεὐχ</td>\n","      <td>16</td>\n","      <td>34 Bodleian-Library-MS-Barocci-37_00118_fol-56...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-874d78ae-c139-4f83-a7cd-79ad484175fa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-874d78ae-c139-4f83-a7cd-79ad484175fa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-874d78ae-c139-4f83-a7cd-79ad484175fa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"zc3qV2-3aDJm"}},{"cell_type":"markdown","source":["### B4: BERT"],"metadata":{"id":"RkIs6BJJIbaN"}},{"cell_type":"markdown","source":["Bibliography:\n","\n","https://arxiv.org/pdf/2002.06823.pdf\n","\n","Bert2Bert:\n","\n","https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=z0FWPCFnCgf6\n"],"metadata":{"id":"qhxVYTwAIbaN"}},{"cell_type":"markdown","source":["#### Load Libraries"],"metadata":{"id":"NHGvSmv2IbaO"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYLZol1_IbaO","executionInfo":{"status":"ok","timestamp":1658773935718,"user_tz":-180,"elapsed":38,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"5944a79a-5951-4f77-eab2-3e20eddcee89"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"267oV8ZvIbaO","executionInfo":{"status":"ok","timestamp":1658773944411,"user_tz":-180,"elapsed":8712,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"34d4b522-4223-409c-82f0-9a85a1a1b015"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install transformers"],"metadata":{"id":"CFmocAf9IbaO","executionInfo":{"status":"ok","timestamp":1658773947863,"user_tz":-180,"elapsed":3468,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### Convert DF to TF"],"metadata":{"id":"sAG1-VUTIbaO"}},{"cell_type":"code","source":["# https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168"],"metadata":{"id":"3tQfgb41IbaO","executionInfo":{"status":"ok","timestamp":1658773947865,"user_tz":-180,"elapsed":19,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_df[[\"HUMAN_TRANSCRIPTION\", \"SYSTEM_TRANSCRIPTION\"]].head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"qB_ykfErIbaP","executionInfo":{"status":"ok","timestamp":1658773947869,"user_tz":-180,"elapsed":20,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"32d13820-fb8f-43d0-897c-d4ac8b3deb1b"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 HUMAN_TRANSCRIPTION  \\\n","0      ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει   \n","1  τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...   \n","2        τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ   \n","\n","                              SYSTEM_TRANSCRIPTION  \n","0         ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει  \n","1  του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν  \n","2           τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα  "],"text/html":["\n","  <div id=\"df-73004a09-6272-495a-a63c-4378be151494\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>HUMAN_TRANSCRIPTION</th>\n","      <th>SYSTEM_TRANSCRIPTION</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει</td>\n","      <td>ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...</td>\n","      <td>του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ</td>\n","      <td>τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73004a09-6272-495a-a63c-4378be151494')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-73004a09-6272-495a-a63c-4378be151494 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-73004a09-6272-495a-a63c-4378be151494');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# training_dataset = (\n","#     tf.data.Dataset.from_tensor_slices(\n","#         (\n","#             tf.cast(train_df[['HUMAN_TRANSCRIPTION', 'SYSTEM_TRANSCRIPTION']].values, tf.string),\n","#         )\n","#     )\n","# )"],"metadata":{"id":"3jYT2pW7IbaP","executionInfo":{"status":"ok","timestamp":1658773947871,"user_tz":-180,"elapsed":19,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#### Tokinizer"],"metadata":{"id":"COI0w_QcIbaP"}},{"cell_type":"code","source":["model_name =  \"nlpaueb/bert-base-greek-uncased-v1\"\n","# model_name =  \"pranaydeeps/Ancient-Greek-BERT\""],"metadata":{"id":"KfNXw5umgaQu","executionInfo":{"status":"ok","timestamp":1658773947873,"user_tz":-180,"elapsed":20,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast\n","# tokenizer = BertTokenizerFast.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n","tokenizer = BertTokenizerFast.from_pretrained(model_name)"],"metadata":{"id":"TY2wOQVBIbaP","executionInfo":{"status":"ok","timestamp":1658773949754,"user_tz":-180,"elapsed":1900,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 120\n","encoder_max_length=MAX_LEN\n","decoder_max_length=MAX_LEN"],"metadata":{"id":"82-tDscRIbaP","executionInfo":{"status":"ok","timestamp":1658773949757,"user_tz":-180,"elapsed":76,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["tokenizer('ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ', padding=\"max_length\", truncation=True,\n","                     max_length=encoder_max_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKASqYnXIbaP","executionInfo":{"status":"ok","timestamp":1658773949761,"user_tz":-180,"elapsed":79,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"f49e46e0-18af-4f8f-b5c6-22e352e6d669"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 2186, 13968, 4785, 6817, 452, 251, 8981, 273, 281, 5429, 374, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens([101, 2186, 13968, 4785, 6817])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAbN87hcIbaP","executionInfo":{"status":"ok","timestamp":1658773949763,"user_tz":-180,"elapsed":76,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"c3f9173b-e74d-4381-c834-70c51dc715a5"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'εγ', '##γιν', '##ομενα', 'παθη']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["tokenizer('ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει', padding=\"max_length\", truncation=True,\n","                     max_length=encoder_max_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxtfmVUMIbaQ","executionInfo":{"status":"ok","timestamp":1658773949767,"user_tz":-180,"elapsed":73,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"748f3b3a-8b4c-4742-eab8-946f60f726f3"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 2186, 5937, 4785, 858, 17903, 10247, 273, 1622, 605, 374, 564, 265, 9822, 532, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens([101, 2186, 5937, 4785, 858, 17903, ])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5PmlvRGIbaQ","executionInfo":{"status":"ok","timestamp":1658773949771,"user_tz":-180,"elapsed":71,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"a481176c-57ec-46f9-94a5-b2ebd7656b31"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'εγ', '##γεν', '##ομενα', '##πα', '##δημη']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["def process_data_to_model_inputs(batch):\n","  # tokenize the inputs and labels\n","  inputs = tokenizer(batch[\"SYSTEM_TRANSCRIPTION\"].values.tolist(), padding=\"max_length\", truncation=True,\n","                     max_length=encoder_max_length)\n","  outputs = tokenizer(batch[\"HUMAN_TRANSCRIPTION\"].values.tolist(), padding=\"max_length\", truncation=True,\n","                      max_length=decoder_max_length)\n","\n","  batch[\"input_ids\"] = inputs.input_ids\n","  batch[\"attention_mask\"] = inputs.attention_mask\n","  batch[\"decoder_input_ids\"] = outputs.input_ids\n","  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n","  batch[\"labels\"] = outputs.input_ids.copy()\n","\n","  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n","  # We have to make sure that the PAD token is ignored\n","  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n","\n","  return batch"],"metadata":{"id":"hWZkvKAWIbaQ","executionInfo":{"status":"ok","timestamp":1658773949774,"user_tz":-180,"elapsed":69,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["train_data = train_df[[\"HUMAN_TRANSCRIPTION\", \"SYSTEM_TRANSCRIPTION\"]]"],"metadata":{"id":"cjahmUN-IbaQ","executionInfo":{"status":"ok","timestamp":1658773949778,"user_tz":-180,"elapsed":72,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["train_data = process_data_to_model_inputs(train_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W4pw8v4pIbaQ","executionInfo":{"status":"ok","timestamp":1658773949787,"user_tz":-180,"elapsed":81,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"c1770a58-9e30-423e-d672-64f027012e3c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if __name__ == '__main__':\n"]}]},{"cell_type":"code","source":["train_data.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"OIJAZxUpIbaQ","executionInfo":{"status":"ok","timestamp":1658773949793,"user_tz":-180,"elapsed":80,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"72b5da2c-6ccd-458f-9e09-d3533911724f"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 HUMAN_TRANSCRIPTION  \\\n","0      ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει   \n","1  τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...   \n","2        τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ   \n","\n","                              SYSTEM_TRANSCRIPTION  \\\n","0         ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει   \n","1  του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν   \n","2           τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα   \n","\n","                                           input_ids  \\\n","0  [101, 2186, 5937, 4785, 858, 17903, 10247, 273...   \n","1  [101, 346, 234, 247, 281, 346, 5608, 5999, 411...   \n","2  [101, 1354, 278, 5120, 8052, 5510, 389, 22396,...   \n","\n","                                      attention_mask  \\\n","0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","\n","                                   decoder_input_ids  \\\n","0  [101, 2186, 13968, 4785, 6817, 452, 251, 8981,...   \n","1  [101, 346, 4466, 346, 1191, 106, 11300, 500, 1...   \n","2  [101, 1354, 278, 5120, 8052, 5510, 3810, 362, ...   \n","\n","                              decoder_attention_mask  \\\n","0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n","2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n","\n","                                              labels  \n","0  [101, 2186, 13968, 4785, 6817, 452, 251, 8981,...  \n","1  [101, 346, 4466, 346, 1191, 106, 11300, 500, 1...  \n","2  [101, 1354, 278, 5120, 8052, 5510, 3810, 362, ...  "],"text/html":["\n","  <div id=\"df-8a0a6288-e509-4a8f-a817-4ae125c1887c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>HUMAN_TRANSCRIPTION</th>\n","      <th>SYSTEM_TRANSCRIPTION</th>\n","      <th>input_ids</th>\n","      <th>attention_mask</th>\n","      <th>decoder_input_ids</th>\n","      <th>decoder_attention_mask</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει</td>\n","      <td>ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει</td>\n","      <td>[101, 2186, 5937, 4785, 858, 17903, 10247, 273...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 2186, 13968, 4785, 6817, 452, 251, 8981,...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 2186, 13968, 4785, 6817, 452, 251, 8981,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...</td>\n","      <td>του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν</td>\n","      <td>[101, 346, 234, 247, 281, 346, 5608, 5999, 411...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 346, 4466, 346, 1191, 106, 11300, 500, 1...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n","      <td>[101, 346, 4466, 346, 1191, 106, 11300, 500, 1...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ</td>\n","      <td>τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα</td>\n","      <td>[101, 1354, 278, 5120, 8052, 5510, 389, 22396,...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 1354, 278, 5120, 8052, 5510, 3810, 362, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n","      <td>[101, 1354, 278, 5120, 8052, 5510, 3810, 362, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a0a6288-e509-4a8f-a817-4ae125c1887c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8a0a6288-e509-4a8f-a817-4ae125c1887c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8a0a6288-e509-4a8f-a817-4ae125c1887c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["val_data = train_data.iloc[:50]\n","val_data.reset_index(inplace=True)\n","\n","test_data = train_data.iloc[50:60]\n","test_data.reset_index(inplace=True)\n","\n","train_data = train_data.iloc[60:1000]\n","train_data.reset_index(inplace=True)"],"metadata":{"id":"4LaWAtmnIbaR","executionInfo":{"status":"ok","timestamp":1658773949798,"user_tz":-180,"elapsed":82,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["print(val_data.shape)\n","print(test_data.shape)\n","print(train_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyRxkoxIIbaR","executionInfo":{"status":"ok","timestamp":1658773949802,"user_tz":-180,"elapsed":86,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"7c231e32-24c0-454a-d986-c29704527a6c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["(50, 8)\n","(10, 8)\n","(940, 8)\n"]}]},{"cell_type":"code","source":["# train_data.drop(columns=['HUMAN_TRANSCRIPTION', 'SYSTEM_TRANSCRIPTION'],inplace=True)"],"metadata":{"id":"T1GCf0TiIbaR","executionInfo":{"status":"ok","timestamp":1658773949805,"user_tz":-180,"elapsed":83,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# item = {key: torch.as_tensor(val) for key, val in train_data.items()}\n","# item"],"metadata":{"id":"Xo9-I9i0IbaR","executionInfo":{"status":"ok","timestamp":1658773949806,"user_tz":-180,"elapsed":83,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class dataset(Dataset):\n","  def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","  def __getitem__(self, index):\n","\n","        # step 1: get the sentence and word labels \n","        sentence = self.data[\"SYSTEM_TRANSCRIPTION\"][index]#.strip().split()  \n","        encoded_labels = self.data[\"labels\"][index]\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        encoding = self.tokenizer(sentence, \n","                                padding=\"max_length\",\n","                                truncation=True,\n","                                max_length=self.max_len)\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","        \n","        return item\n","\n","  def __len__(self):\n","        return self.len"],"metadata":{"id":"kHcRgzM0IbaR","executionInfo":{"status":"ok","timestamp":1658773949807,"user_tz":-180,"elapsed":83,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-RJkhfMpIbaR","executionInfo":{"status":"ok","timestamp":1658773949808,"user_tz":-180,"elapsed":84,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["training_set = dataset(train_data, tokenizer, MAX_LEN)\n","validation_set = dataset(val_data, tokenizer, MAX_LEN)\n","test_set = dataset(test_data, tokenizer, MAX_LEN)"],"metadata":{"id":"LrTd_H6sIbaR","executionInfo":{"status":"ok","timestamp":1658773949809,"user_tz":-180,"elapsed":84,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["training_set[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0_jZ4pnIbaS","executionInfo":{"status":"ok","timestamp":1658773949810,"user_tz":-180,"elapsed":84,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"c4846c40-f635-46d7-f507-b9e0da46cd12"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'input_ids': tensor([  101,  3258, 13881,  1987,   269,   344,   355,  6186,  4993,   404,\n","         10012,   265,   267,  5668,   275,   267,  8276,  6298,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n"," 'labels': tensor([  101,  3258,  9959,   269,   344, 13629,   532,   247,   240,   278,\n","         29869,   278,  6298,   102,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]),\n"," 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# train_data.set_format(\n","#     type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n","# )"],"metadata":{"id":"1MxjtKB2IbaS","executionInfo":{"status":"ok","timestamp":1658773949811,"user_tz":-180,"elapsed":80,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# val_data = val_data.map(\n","#     process_data_to_model_inputs, \n","#     batched=True, \n","#     batch_size=batch_size, \n","#     remove_columns=[\"article\", \"highlights\", \"id\"]\n","# )"],"metadata":{"id":"kLntZ-h3IbaS","executionInfo":{"status":"ok","timestamp":1658773949812,"user_tz":-180,"elapsed":80,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# val_data.set_format(\n","#     type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n","# )"],"metadata":{"id":"O54wDC1oIbaS","executionInfo":{"status":"ok","timestamp":1658773949813,"user_tz":-180,"elapsed":80,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["#### Model"],"metadata":{"id":"_J-nA8rOIbaS"}},{"cell_type":"markdown","source":["\n","Hugging Face Bert:\n","\n","https://huggingface.co/pranaydeeps/Ancient-Greek-BERT\n","\n","OR\n","(https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1)"],"metadata":{"id":"FlfiH1OcIbaS"}},{"cell_type":"code","source":["from transformers import EncoderDecoderModel\n","# bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"pranaydeeps/Ancient-Greek-BERT\", \"pranaydeeps/Ancient-Greek-BERT\")\n","bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"735VaeklIbaS","executionInfo":{"status":"ok","timestamp":1658773953694,"user_tz":-180,"elapsed":3960,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"0a8e9830-c5e3-46ee-c0fd-ac1aba42ca1c"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at nlpaueb/bert-base-greek-uncased-v1 and are newly initialized: ['bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from transformers import BertGenerationEncoder,EncoderDecoderModel,BertGenerationDecoder\n","# # leverage checkpoints for Bert2Bert model...\n","# # use BERT's cls token as BOS token and sep token as EOS token\n","# encoder = BertGenerationEncoder.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\", \n","#                                                 bos_token_id=101,\n","#                                                 eos_token_id=102)\n","# # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n","# decoder = BertGenerationDecoder.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\",\n","#                                                 add_cross_attention=True,\n","#                                                 is_decoder=True, \n","#                                                 bos_token_id=101, \n","#                                                 eos_token_id=102)\n","# bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n"],"metadata":{"id":"0oJTCD2ZIbaT","executionInfo":{"status":"ok","timestamp":1658773953700,"user_tz":-180,"elapsed":60,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["bert2bert.to('cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SharoeOjIbaT","executionInfo":{"status":"ok","timestamp":1658773954733,"user_tz":-180,"elapsed":1074,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"167587e1-7b16-4231-901d-9c82fa7f7922"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoderModel(\n","  (encoder): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (decoder): BertLMHeadModel(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (cls): BertOnlyMLMHead(\n","      (predictions): BertLMPredictionHead(\n","        (transform): BertPredictionHeadTransform(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (transform_act_fn): GELUActivation()\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (decoder): Linear(in_features=768, out_features=35000, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvqPB-sjIbaT","executionInfo":{"status":"ok","timestamp":1658773954737,"user_tz":-180,"elapsed":216,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"f37a28c7-9640-4ac8-d613-78d1ffa3e44f"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Jul 25 18:32:33 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    27W /  70W |   1858MiB / 15109MiB |      3%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["bert2bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybuH8MAcIbaT","executionInfo":{"status":"ok","timestamp":1658773955163,"user_tz":-180,"elapsed":448,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"1944c0ad-d481-4490-bf2c-284bc31a9e0d"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EncoderDecoderModel(\n","  (encoder): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (decoder): BertLMHeadModel(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(35000, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (cls): BertOnlyMLMHead(\n","      (predictions): BertLMPredictionHead(\n","        (transform): BertPredictionHeadTransform(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (transform_act_fn): GELUActivation()\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (decoder): Linear(in_features=768, out_features=35000, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# bert2bert.save_pretrained(\"bert2bert\")"],"metadata":{"id":"QnEvtjUlIbaT","executionInfo":{"status":"ok","timestamp":1658773955165,"user_tz":-180,"elapsed":37,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainer\n","from transformers import Seq2SeqTrainingArguments\n","from transformers import EarlyStoppingCallback"],"metadata":{"id":"2t7jLxj_IbaT","executionInfo":{"status":"ok","timestamp":1658773955169,"user_tz":-180,"elapsed":39,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["batch_size = 32"],"metadata":{"id":"Pjkl5GGxIbaU","executionInfo":{"status":"ok","timestamp":1658773955171,"user_tz":-180,"elapsed":38,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n","bert2bert.config.eos_token_id = tokenizer.sep_token_id\n","bert2bert.config.pad_token_id = tokenizer.pad_token_id\n","bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size"],"metadata":{"id":"DoVpvpOKIbaU","executionInfo":{"status":"ok","timestamp":1658773955173,"user_tz":-180,"elapsed":36,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["bert2bert.config.max_length = 120\n","bert2bert.config.min_length = 120\n","# bert2bert.config.no_repeat_ngram_size = 3\n","bert2bert.config.early_stopping = True\n","# bert2bert.config.length_penalty = 2.0\n","# bert2bert.config.num_beams = 4"],"metadata":{"id":"a0ICGiVlIbaU","executionInfo":{"status":"ok","timestamp":1658773955174,"user_tz":-180,"elapsed":35,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["training_args = Seq2SeqTrainingArguments(\n","    predict_with_generate=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    fp16=True, \n","    warmup_steps=1000,\n","    learning_rate=2e-4,\n","    seed=0,\n","    output_dir=\"./\",\n","    logging_steps=2,\n","    save_steps=4,\n","    num_train_epochs=25,\n","    eval_steps = 4, # Evaluation and Save happens every 10 steps\n","    save_total_limit = 2, # Only last 5 models are saved. Older ones are deleted.\n","    load_best_model_at_end=True,\n","    metric_for_best_model = \"loss\",\n","    # logging_steps=1000,\n","    # save_steps=500,\n","    # eval_steps=7500,\n","    # warmup_steps=2000,\n","    # save_total_limit=3,\n",")"],"metadata":{"id":"v1v1fvjlIbaU","executionInfo":{"status":"ok","timestamp":1658773955179,"user_tz":-180,"elapsed":38,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install git-python==1.0.3\n","!pip install rouge_score\n","!pip install sacrebleu\n","!pip install datasets==1.0.2"],"metadata":{"id":"8LFpRSwyIbaU","executionInfo":{"status":"ok","timestamp":1658773977074,"user_tz":-180,"elapsed":21930,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["import datasets"],"metadata":{"id":"VaEgnz3uIbaU","executionInfo":{"status":"ok","timestamp":1658773977075,"user_tz":-180,"elapsed":35,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["rouge = datasets.load_metric(\"rouge\") # metric"],"metadata":{"id":"xaiNWFmCIbaU","executionInfo":{"status":"ok","timestamp":1658773978570,"user_tz":-180,"elapsed":1527,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n","\n","    return {\n","        \"rouge2_precision\": round(rouge_output.precision, 4),\n","        \"rouge2_recall\": round(rouge_output.recall, 4),\n","        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n","    }"],"metadata":{"id":"Qx7-jS-rIbaV","executionInfo":{"status":"ok","timestamp":1658773978572,"user_tz":-180,"elapsed":16,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install wanb"],"metadata":{"id":"CwYoy8U-edC2","executionInfo":{"status":"ok","timestamp":1658773980343,"user_tz":-180,"elapsed":1293,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# instantiate trainer\n","\n","trainer = Seq2SeqTrainer(\n","    model=bert2bert,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=training_set,#train_data,\n","    eval_dataset=validation_set,#val_data,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",")\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UIdUlA4GIbaV","executionInfo":{"status":"ok","timestamp":1658777231071,"user_tz":-180,"elapsed":3250747,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"aeba851f-99d7-4bc4-a892-2a4a4d4f3046"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cuda_amp half precision backend\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 940\n","  Num Epochs = 25\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 750\n","The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='220' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [220/750 54:08 < 2:11:36, 0.07 it/s, Epoch 7/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge2 Precision</th>\n","      <th>Rouge2 Recall</th>\n","      <th>Rouge2 Fmeasure</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>4</td>\n","      <td>13.561800</td>\n","      <td>14.739798</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>13.028100</td>\n","      <td>14.118910</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>12.240900</td>\n","      <td>12.251154</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>10.854400</td>\n","      <td>10.042762</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>9.719900</td>\n","      <td>8.902678</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>8.752100</td>\n","      <td>8.293139</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>8.218800</td>\n","      <td>7.993489</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>7.800400</td>\n","      <td>7.827537</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>7.628700</td>\n","      <td>7.616158</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>7.380200</td>\n","      <td>7.348464</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>7.109900</td>\n","      <td>7.018889</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>6.803200</td>\n","      <td>6.920640</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>6.606000</td>\n","      <td>6.756159</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>6.479000</td>\n","      <td>6.635400</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>6.320700</td>\n","      <td>6.545327</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>6.276800</td>\n","      <td>6.428393</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>6.202300</td>\n","      <td>6.402174</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>6.096100</td>\n","      <td>6.395377</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>6.065700</td>\n","      <td>6.370840</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>5.960100</td>\n","      <td>6.338660</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>5.882100</td>\n","      <td>6.340374</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>5.990300</td>\n","      <td>6.204734</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>5.738400</td>\n","      <td>6.209869</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>5.882200</td>\n","      <td>6.136760</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>5.755100</td>\n","      <td>6.035252</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>5.725800</td>\n","      <td>6.001485</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>5.514500</td>\n","      <td>5.906728</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>5.603400</td>\n","      <td>5.956113</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>5.599800</td>\n","      <td>5.822583</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>5.477700</td>\n","      <td>5.747776</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>5.384100</td>\n","      <td>5.709095</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>5.368600</td>\n","      <td>5.731770</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>5.283500</td>\n","      <td>5.610404</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>5.333600</td>\n","      <td>5.636267</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>5.299800</td>\n","      <td>5.672793</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>5.339500</td>\n","      <td>5.560731</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>5.251800</td>\n","      <td>5.508523</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>5.077600</td>\n","      <td>5.508788</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>5.052300</td>\n","      <td>5.443121</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>4.960100</td>\n","      <td>5.456512</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>4.900100</td>\n","      <td>5.420718</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>4.823700</td>\n","      <td>5.418403</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>4.945900</td>\n","      <td>5.418581</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>176</td>\n","      <td>4.898600</td>\n","      <td>5.380347</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>4.859100</td>\n","      <td>5.329627</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>184</td>\n","      <td>4.598000</td>\n","      <td>5.430163</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>188</td>\n","      <td>4.637400</td>\n","      <td>5.513493</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>192</td>\n","      <td>4.610800</td>\n","      <td>5.394254</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>196</td>\n","      <td>4.658700</td>\n","      <td>5.365191</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>4.616900</td>\n","      <td>5.306644</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>204</td>\n","      <td>4.672600</td>\n","      <td>5.332593</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>208</td>\n","      <td>4.729000</td>\n","      <td>5.308857</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>212</td>\n","      <td>4.194100</td>\n","      <td>5.401154</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>216</td>\n","      <td>4.288400</td>\n","      <td>5.459810</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>4.210200</td>\n","      <td>5.345192</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-4\n","Configuration saved in ./checkpoint-4/config.json\n","Model weights saved in ./checkpoint-4/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-100] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-8\n","Configuration saved in ./checkpoint-8/config.json\n","Model weights saved in ./checkpoint-8/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-112] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-12\n","Configuration saved in ./checkpoint-12/config.json\n","Model weights saved in ./checkpoint-12/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-4] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-16\n","Configuration saved in ./checkpoint-16/config.json\n","Model weights saved in ./checkpoint-16/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-8] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-20\n","Configuration saved in ./checkpoint-20/config.json\n","Model weights saved in ./checkpoint-20/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-12] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-24\n","Configuration saved in ./checkpoint-24/config.json\n","Model weights saved in ./checkpoint-24/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-16] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-28\n","Configuration saved in ./checkpoint-28/config.json\n","Model weights saved in ./checkpoint-28/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-32\n","Configuration saved in ./checkpoint-32/config.json\n","Model weights saved in ./checkpoint-32/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-24] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-36\n","Configuration saved in ./checkpoint-36/config.json\n","Model weights saved in ./checkpoint-36/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-28] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-40\n","Configuration saved in ./checkpoint-40/config.json\n","Model weights saved in ./checkpoint-40/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-32] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-44\n","Configuration saved in ./checkpoint-44/config.json\n","Model weights saved in ./checkpoint-44/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-36] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-48\n","Configuration saved in ./checkpoint-48/config.json\n","Model weights saved in ./checkpoint-48/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-52\n","Configuration saved in ./checkpoint-52/config.json\n","Model weights saved in ./checkpoint-52/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-44] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-56\n","Configuration saved in ./checkpoint-56/config.json\n","Model weights saved in ./checkpoint-56/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-48] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-60\n","Configuration saved in ./checkpoint-60/config.json\n","Model weights saved in ./checkpoint-60/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-52] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-64\n","Configuration saved in ./checkpoint-64/config.json\n","Model weights saved in ./checkpoint-64/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-56] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-68\n","Configuration saved in ./checkpoint-68/config.json\n","Model weights saved in ./checkpoint-68/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-72\n","Configuration saved in ./checkpoint-72/config.json\n","Model weights saved in ./checkpoint-72/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-64] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-76\n","Configuration saved in ./checkpoint-76/config.json\n","Model weights saved in ./checkpoint-76/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-68] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-80\n","Configuration saved in ./checkpoint-80/config.json\n","Model weights saved in ./checkpoint-80/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-72] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-84\n","Configuration saved in ./checkpoint-84/config.json\n","Model weights saved in ./checkpoint-84/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-76] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-88\n","Configuration saved in ./checkpoint-88/config.json\n","Model weights saved in ./checkpoint-88/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-92\n","Configuration saved in ./checkpoint-92/config.json\n","Model weights saved in ./checkpoint-92/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-84] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-96\n","Configuration saved in ./checkpoint-96/config.json\n","Model weights saved in ./checkpoint-96/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-88] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-100\n","Configuration saved in ./checkpoint-100/config.json\n","Model weights saved in ./checkpoint-100/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-92] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-104\n","Configuration saved in ./checkpoint-104/config.json\n","Model weights saved in ./checkpoint-104/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-96] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-108\n","Configuration saved in ./checkpoint-108/config.json\n","Model weights saved in ./checkpoint-108/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-100] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-112\n","Configuration saved in ./checkpoint-112/config.json\n","Model weights saved in ./checkpoint-112/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-104] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-116\n","Configuration saved in ./checkpoint-116/config.json\n","Model weights saved in ./checkpoint-116/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-108] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-120\n","Configuration saved in ./checkpoint-120/config.json\n","Model weights saved in ./checkpoint-120/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-112] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-124\n","Configuration saved in ./checkpoint-124/config.json\n","Model weights saved in ./checkpoint-124/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-116] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-128\n","Configuration saved in ./checkpoint-128/config.json\n","Model weights saved in ./checkpoint-128/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-120] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-132\n","Configuration saved in ./checkpoint-132/config.json\n","Model weights saved in ./checkpoint-132/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-124] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-136\n","Configuration saved in ./checkpoint-136/config.json\n","Model weights saved in ./checkpoint-136/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-128] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-140\n","Configuration saved in ./checkpoint-140/config.json\n","Model weights saved in ./checkpoint-140/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-136] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-144\n","Configuration saved in ./checkpoint-144/config.json\n","Model weights saved in ./checkpoint-144/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-132] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-148\n","Configuration saved in ./checkpoint-148/config.json\n","Model weights saved in ./checkpoint-148/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-140] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-152\n","Configuration saved in ./checkpoint-152/config.json\n","Model weights saved in ./checkpoint-152/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-144] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-156\n","Configuration saved in ./checkpoint-156/config.json\n","Model weights saved in ./checkpoint-156/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-148] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-160\n","Configuration saved in ./checkpoint-160/config.json\n","Model weights saved in ./checkpoint-160/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-152] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-164\n","Configuration saved in ./checkpoint-164/config.json\n","Model weights saved in ./checkpoint-164/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-156] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-168\n","Configuration saved in ./checkpoint-168/config.json\n","Model weights saved in ./checkpoint-168/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-160] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-172\n","Configuration saved in ./checkpoint-172/config.json\n","Model weights saved in ./checkpoint-172/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-164] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-176\n","Configuration saved in ./checkpoint-176/config.json\n","Model weights saved in ./checkpoint-176/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-168] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-180\n","Configuration saved in ./checkpoint-180/config.json\n","Model weights saved in ./checkpoint-180/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-172] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-184\n","Configuration saved in ./checkpoint-184/config.json\n","Model weights saved in ./checkpoint-184/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-176] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-188\n","Configuration saved in ./checkpoint-188/config.json\n","Model weights saved in ./checkpoint-188/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-184] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-192\n","Configuration saved in ./checkpoint-192/config.json\n","Model weights saved in ./checkpoint-192/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-188] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-196\n","Configuration saved in ./checkpoint-196/config.json\n","Model weights saved in ./checkpoint-196/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-192] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-200\n","Configuration saved in ./checkpoint-200/config.json\n","Model weights saved in ./checkpoint-200/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-180] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-204\n","Configuration saved in ./checkpoint-204/config.json\n","Model weights saved in ./checkpoint-204/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-196] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-208\n","Configuration saved in ./checkpoint-208/config.json\n","Model weights saved in ./checkpoint-208/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-204] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-212\n","Configuration saved in ./checkpoint-212/config.json\n","Model weights saved in ./checkpoint-212/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-208] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-216\n","Configuration saved in ./checkpoint-216/config.json\n","Model weights saved in ./checkpoint-216/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-212] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 32\n","The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n","Saving model checkpoint to ./checkpoint-220\n","Configuration saved in ./checkpoint-220/config.json\n","Model weights saved in ./checkpoint-220/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-216] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./checkpoint-200 (score: 5.306644439697266).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=220, training_loss=6.264200774106112, metrics={'train_runtime': 3250.7058, 'train_samples_per_second': 7.229, 'train_steps_per_second': 0.231, 'total_flos': 992092549824000.0, 'train_loss': 6.264200774106112, 'epoch': 7.33})"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"w6iJ1vqTIbbp"}},{"cell_type":"code","source":[""],"metadata":{"id":"_PvaSdwjIbbp","executionInfo":{"status":"ok","timestamp":1658777231073,"user_tz":-180,"elapsed":36,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["def generate_summary(batch):\n","    # cut off at BERT max length 512\n","    inputs = tokenizer(batch[\"SYSTEM_TRANSCRIPTION\"].to_list(), padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","    input_ids = inputs.input_ids.to(\"cuda\")\n","    attention_mask = inputs.attention_mask.to(\"cuda\")\n","\n","    outputs = bert2bert.generate(input_ids, attention_mask=attention_mask)\n","\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    batch[\"pred_summary\"] = output_str\n","\n","    return batch"],"metadata":{"id":"zyvVeyFkIbbp","executionInfo":{"status":"ok","timestamp":1658777231074,"user_tz":-180,"elapsed":32,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["batch = generate_summary(test_data)"],"metadata":{"id":"GbnrkCspIbbp","executionInfo":{"status":"ok","timestamp":1658777233905,"user_tz":-180,"elapsed":2860,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"id":"0AcY9ogIIbbq","executionInfo":{"status":"ok","timestamp":1658777234427,"user_tz":-180,"elapsed":17,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"0e1e1a0c-c8d8-4974-a981-270e2de23291"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   index                          HUMAN_TRANSCRIPTION  \\\n","0     50        σχυνοντες οποιος ην πτωχος ιωαννης μη   \n","1     51     οικον εχων μη οικετην μη βουν αροτηρα μη   \n","2     52        γηδιον μη κλινην μη τραπεζαν μη αρτον   \n","3     53        οποιος ηλιας οποιος των αγιων εκαστος   \n","4     54   οι περιηλθον εν μηλωταις εν αιγειοις δερμα   \n","5     55    σιν υστερουμενοι θλιβομενοι κακουχουμενοι   \n","6     56      ανθ ων υψωθησαν αι θυγατερες σιων και ε   \n","7     57        πορευθησαν υψηλω τραχηλω και εν νευμα   \n","8     58       σιν οφθαλμων και τη πορεια των ποδων α   \n","9     59  μα συρουσαι τους χιτωνας και τοις ποσιν αμα   \n","\n","                         SYSTEM_TRANSCRIPTION  \\\n","0       εχινοντεσ όποιος ηνπτωχος ιωαλνης ἐμη   \n","1    οικον χων όμε οικετην ύβηβον αροτηραταει   \n","2        χηδιον ψιξλένην λη τραυπεδαν μκαρτόν   \n","3        εποιος ψγίας εποιος των αγιων εκασος   \n","4      οιπεριἡλθον εν μηλωταις εναιγίοι σδερα   \n","5      σιο ὑπτερ ουμενοι θυκομεν οικακουχουεν   \n","6       ζθῶν ζυθης ακ ἀθυτα τερες εία ἡ και ε   \n","7        πορεύθηξακτοψηλωτρ χήλω και εμν εκμα   \n","8     ςίκε ὁ φθάλνα καὶ τὴ πορειατωη ποδυων α   \n","9  μαςύρογκαρ τον εχιτω ας κα τοις πος ιη αμα   \n","\n","                                           input_ids  \\\n","0  [101, 4569, 2658, 1907, 2781, 549, 4755, 1423,...   \n","1  [101, 10302, 7513, 273, 247, 438, 5850, 6338, ...   \n","2  [101, 255, 267, 2927, 273, 256, 269, 274, 6999...   \n","3  [101, 3830, 2251, 256, 9847, 3830, 2251, 358, ...   \n","4  [101, 363, 8024, 31108, 437, 452, 1480, 497, 2...   \n","5  [101, 1580, 275, 995, 1645, 247, 5094, 2313, 6...   \n","6  [101, 238, 6019, 238, 11883, 415, 3162, 8618, ...   \n","7  [101, 12868, 902, 1698, 5299, 3414, 1480, 6847...   \n","8  [101, 250, 2814, 265, 247, 10580, 271, 521, 34...   \n","9  [101, 371, 6509, 12720, 362, 4569, 3001, 285, ...   \n","\n","                                      attention_mask  \\\n","0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n","4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","\n","                                   decoder_input_ids  \\\n","0  [101, 6301, 281, 11836, 605, 549, 239, 273, 18...   \n","1  [101, 10302, 576, 273, 452, 5850, 6338, 273, 4...   \n","2  [101, 1244, 2927, 273, 452, 12341, 2389, 452, ...   \n","3  [101, 549, 5966, 549, 358, 4141, 12856, 278, 1...   \n","4  [101, 363, 518, 31108, 437, 452, 1480, 497, 27...   \n","5  [101, 1580, 273, 15342, 5094, 2313, 22276, 567...   \n","6  [101, 5148, 257, 273, 6485, 7641, 546, 1090, 2...   \n","7  [101, 12868, 7683, 13235, 1480, 13326, 3714, 2...   \n","8  [101, 1580, 273, 33284, 344, 365, 903, 358, 20...   \n","9  [101, 656, 10468, 2705, 364, 31343, 278, 344, ...   \n","\n","                              decoder_attention_mask  \\\n","0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n","1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n","3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n","4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n","6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n","8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n","9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n","\n","                                              labels  \\\n","0  [101, 6301, 281, 11836, 605, 549, 239, 273, 18...   \n","1  [101, 10302, 576, 273, 452, 5850, 6338, 273, 4...   \n","2  [101, 1244, 2927, 273, 452, 12341, 2389, 452, ...   \n","3  [101, 549, 5966, 549, 358, 4141, 12856, 278, 1...   \n","4  [101, 363, 518, 31108, 437, 452, 1480, 497, 27...   \n","5  [101, 1580, 273, 15342, 5094, 2313, 22276, 567...   \n","6  [101, 5148, 257, 273, 6485, 7641, 546, 1090, 2...   \n","7  [101, 12868, 7683, 13235, 1480, 13326, 3714, 2...   \n","8  [101, 1580, 273, 33284, 344, 365, 903, 358, 20...   \n","9  [101, 656, 10468, 2705, 364, 31343, 278, 344, ...   \n","\n","                                        pred_summary  \n","0  ω ω ω ω ω ω ω ω ω ω ω ω ω ω ως, ω ω ως, ω ω ως...  \n","1  ω ω ω ω ω ω ω ω ω ω ω ω, ω ω, ω ω, ω ω, ω ω, ω...  \n","2  , ω ω ω ω ω ω ω ω ω..............................  \n","3  ος ος ος ος ος ος ος ος ος, ος, ος, ος,, ους, ...  \n","4  οντες εν τοις τοις οφθαλμοις εν τοις τοις τοις...  \n","5  ουυν ουν ουν ουν. ουν.. ουυν.. ουυν. ουυν.. ου...  \n","6  ης ης ης ης ης ης ης ης ης ης ης ης ης ης ης η...  \n","7  και ως ω ω ω ω ωθεν. και ω ωθεν, ω ω ω ωθεν, ω...  \n","8  και ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω. και...  \n","9  και ταδεδεδε και και ταδεδεδε και και το σχημα...  "],"text/html":["\n","  <div id=\"df-70d99d60-fd19-4543-b392-f52e6580770c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>HUMAN_TRANSCRIPTION</th>\n","      <th>SYSTEM_TRANSCRIPTION</th>\n","      <th>input_ids</th>\n","      <th>attention_mask</th>\n","      <th>decoder_input_ids</th>\n","      <th>decoder_attention_mask</th>\n","      <th>labels</th>\n","      <th>pred_summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>50</td>\n","      <td>σχυνοντες οποιος ην πτωχος ιωαννης μη</td>\n","      <td>εχινοντεσ όποιος ηνπτωχος ιωαλνης ἐμη</td>\n","      <td>[101, 4569, 2658, 1907, 2781, 549, 4755, 1423,...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 6301, 281, 11836, 605, 549, 239, 273, 18...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n","      <td>[101, 6301, 281, 11836, 605, 549, 239, 273, 18...</td>\n","      <td>ω ω ω ω ω ω ω ω ω ω ω ω ω ω ως, ω ω ως, ω ω ως...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>51</td>\n","      <td>οικον εχων μη οικετην μη βουν αροτηρα μη</td>\n","      <td>οικον χων όμε οικετην ύβηβον αροτηραταει</td>\n","      <td>[101, 10302, 7513, 273, 247, 438, 5850, 6338, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 10302, 576, 273, 452, 5850, 6338, 273, 4...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 10302, 576, 273, 452, 5850, 6338, 273, 4...</td>\n","      <td>ω ω ω ω ω ω ω ω ω ω ω ω, ω ω, ω ω, ω ω, ω ω, ω...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>52</td>\n","      <td>γηδιον μη κλινην μη τραπεζαν μη αρτον</td>\n","      <td>χηδιον ψιξλένην λη τραυπεδαν μκαρτόν</td>\n","      <td>[101, 255, 267, 2927, 273, 256, 269, 274, 6999...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 1244, 2927, 273, 452, 12341, 2389, 452, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n","      <td>[101, 1244, 2927, 273, 452, 12341, 2389, 452, ...</td>\n","      <td>, ω ω ω ω ω ω ω ω ω..............................</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>οποιος ηλιας οποιος των αγιων εκαστος</td>\n","      <td>εποιος ψγίας εποιος των αγιων εκασος</td>\n","      <td>[101, 3830, 2251, 256, 9847, 3830, 2251, 358, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n","      <td>[101, 549, 5966, 549, 358, 4141, 12856, 278, 1...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[101, 549, 5966, 549, 358, 4141, 12856, 278, 1...</td>\n","      <td>ος ος ος ος ος ος ος ος ος, ος, ος, ος,, ους, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54</td>\n","      <td>οι περιηλθον εν μηλωταις εν αιγειοις δερμα</td>\n","      <td>οιπεριἡλθον εν μηλωταις εναιγίοι σδερα</td>\n","      <td>[101, 363, 8024, 31108, 437, 452, 1480, 497, 2...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 363, 518, 31108, 437, 452, 1480, 497, 27...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 363, 518, 31108, 437, 452, 1480, 497, 27...</td>\n","      <td>οντες εν τοις τοις οφθαλμοις εν τοις τοις τοις...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>55</td>\n","      <td>σιν υστερουμενοι θλιβομενοι κακουχουμενοι</td>\n","      <td>σιο ὑπτερ ουμενοι θυκομεν οικακουχουεν</td>\n","      <td>[101, 1580, 275, 995, 1645, 247, 5094, 2313, 6...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 1580, 273, 15342, 5094, 2313, 22276, 567...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n","      <td>[101, 1580, 273, 15342, 5094, 2313, 22276, 567...</td>\n","      <td>ουυν ουν ουν ουν. ουν.. ουυν.. ουυν. ουυν.. ου...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>56</td>\n","      <td>ανθ ων υψωθησαν αι θυγατερες σιων και ε</td>\n","      <td>ζθῶν ζυθης ακ ἀθυτα τερες εία ἡ και ε</td>\n","      <td>[101, 238, 6019, 238, 11883, 415, 3162, 8618, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 5148, 257, 273, 6485, 7641, 546, 1090, 2...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 5148, 257, 273, 6485, 7641, 546, 1090, 2...</td>\n","      <td>ης ης ης ης ης ης ης ης ης ης ης ης ης ης ης η...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>57</td>\n","      <td>πορευθησαν υψηλω τραχηλω και εν νευμα</td>\n","      <td>πορεύθηξακτοψηλωτρ χήλω και εμν εκμα</td>\n","      <td>[101, 12868, 902, 1698, 5299, 3414, 1480, 6847...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 12868, 7683, 13235, 1480, 13326, 3714, 2...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n","      <td>[101, 12868, 7683, 13235, 1480, 13326, 3714, 2...</td>\n","      <td>και ως ω ω ω ω ωθεν. και ω ωθεν, ω ω ω ωθεν, ω...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>58</td>\n","      <td>σιν οφθαλμων και τη πορεια των ποδων α</td>\n","      <td>ςίκε ὁ φθάλνα καὶ τὴ πορειατωη ποδυων α</td>\n","      <td>[101, 250, 2814, 265, 247, 10580, 271, 521, 34...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 1580, 273, 33284, 344, 365, 903, 358, 20...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n","      <td>[101, 1580, 273, 33284, 344, 365, 903, 358, 20...</td>\n","      <td>και ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω ω. και...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>59</td>\n","      <td>μα συρουσαι τους χιτωνας και τοις ποσιν αμα</td>\n","      <td>μαςύρογκαρ τον εχιτω ας κα τοις πος ιη αμα</td>\n","      <td>[101, 371, 6509, 12720, 362, 4569, 3001, 285, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[101, 656, 10468, 2705, 364, 31343, 278, 344, ...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n","      <td>[101, 656, 10468, 2705, 364, 31343, 278, 344, ...</td>\n","      <td>και ταδεδεδε και και ταδεδεδε και και το σχημα...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70d99d60-fd19-4543-b392-f52e6580770c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-70d99d60-fd19-4543-b392-f52e6580770c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-70d99d60-fd19-4543-b392-f52e6580770c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["batch_size = 16  # change to 64 for full evaluation\n","\n","results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"id":"TinQUvHAIbbq","executionInfo":{"status":"error","timestamp":1658777236373,"user_tz":-180,"elapsed":1957,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}},"outputId":"d2a04b00-4f9d-4de9-8988-23cd9263aeca"},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n","\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.7/dist-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m2882\u001b[0m in \u001b[92mrun_code\u001b[0m  \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m2879 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                   \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m2880 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.hooks.pre_run_code_hook()                                     \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m2881 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m                      \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m2882 \u001b[2m│   │   │   │   \u001b[0mexec(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)                  \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m2883 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                               \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m2884 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Reset our crash handler in place\u001b[0m                                 \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m2885 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msys.excepthook = old_excepthook                                    \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m \u001b[33m<ipython-input-54-1b60aa113372>\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                                             \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.7/dist-packages/pandas/core/\u001b[0m\u001b[1;33mgeneric.py\u001b[0m:\u001b[94m5487\u001b[0m in \u001b[92m__getattr__\u001b[0m         \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m 5484 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[96mself\u001b[0m._info_axis._can_hold_identifiers_and_holds_name(name)        \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m 5485 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                                        \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m 5486 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m[name]                                                     \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 5487 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mobject\u001b[0m.\u001b[92m__getattribute__\u001b[0m(\u001b[96mself\u001b[0m, name)                                \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m 5488 \u001b[0m\u001b[2m│   \u001b[0m                                                                              \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m 5489 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__setattr__\u001b[0m(\u001b[96mself\u001b[0m, name: \u001b[96mstr\u001b[0m, value) -> \u001b[94mNone\u001b[0m:                              \u001b[91m│\u001b[0m\n","\u001b[91m│\u001b[0m   \u001b[2m 5490 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                       \u001b[91m│\u001b[0m\n","\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'DataFrame'\u001b[0m object has no attribute \u001b[32m'map'\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.7/dist-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2882</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_code</span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2879 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2880 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hooks.pre_run_code_hook()                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2881 │   │   │   │   #rprint('Running code', repr(code_obj)) # dbg</span>                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2882 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exec(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2883 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2884 │   │   │   │   # Reset our crash handler in place</span>                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2885 │   │   │   │   </span>sys.excepthook = old_excepthook                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-54-1b60aa113372&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.7/dist-packages/pandas/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">generic.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5484 │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._info_axis._can_hold_identifiers_and_holds_name(name)        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5485 │   │   </span>):                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5486 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>[name]                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">object</span>.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattribute__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name)                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5488 │   </span>                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5489 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setattr__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, value) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5490 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'DataFrame'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'map'</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["rouge.compute(predictions=results[\"pred_summary\"], references=results[\"highlights\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid"],"metadata":{"id":"GAC_cYGTIbbq","executionInfo":{"status":"aborted","timestamp":1658777235685,"user_tz":-180,"elapsed":45,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"rAhiNupcIbbq"}},{"cell_type":"code","source":["# # create tokenizer...\n","# tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n","\n","# input_ids = tokenizer(\n","#     \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n","# labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n","\n","# # train...\n","# loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n","# loss.backward()"],"metadata":{"id":"FmLl3bXBIbbq","executionInfo":{"status":"aborted","timestamp":1658777235687,"user_tz":-180,"elapsed":46,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_s7dKt1nIbbr","executionInfo":{"status":"aborted","timestamp":1658777235688,"user_tz":-180,"elapsed":47,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import BertTokenizer\n","\n","# # Load the BERT tokenizer.\n","# print('Loading BERT tokenizer...')\n","# # tokenizer = BertTokenizer.from_pretrained('pranaydeeps/Ancient-Greek-BERT', do_lower_case=True)\n","# tokenizer = BertTokenizer.from_pretrained('pranaydeeps/Ancient-Greek-BERT')"],"metadata":{"id":"dmmYOlB7Ibbr","executionInfo":{"status":"aborted","timestamp":1658777235689,"user_tz":-180,"elapsed":48,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sentences = train_df.SYSTEM_TRANSCRIPTION\n","# sentences[0]"],"metadata":{"id":"AQcBUEPkIbbr","executionInfo":{"status":"aborted","timestamp":1658777235690,"user_tz":-180,"elapsed":49,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Print the original sentence.\n","# print(' Original: ', sentences[0])\n","\n","# # Print the sentence split into tokens.\n","# print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# # Print the sentence mapped to token ids.\n","# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"metadata":{"id":"8ADYM9c3Ibbr","executionInfo":{"status":"aborted","timestamp":1658777235692,"user_tz":-180,"elapsed":51,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Tokenize the dataset"],"metadata":{"id":"-x_pCQ5OIbbr"}},{"cell_type":"code","source":["# # Tokenize all of the sentences and map the tokens to thier word IDs.\n","# input_ids = []\n","# attention_masks = []\n","\n","# # For every sentence...\n","# for sent in sentences:\n","#     # `encode_plus` will:\n","#     #   (1) Tokenize the sentence.\n","#     #   (2) Prepend the `[CLS]` token to the start.\n","#     #   (3) Append the `[SEP]` token to the end.\n","#     #   (4) Map tokens to their IDs.\n","#     #   (5) Pad or truncate the sentence to `max_length`\n","#     #   (6) Create attention masks for [PAD] tokens.\n","#     encoded_dict = tokenizer.encode_plus(\n","#                         sent,                      # Sentence to encode.\n","#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","#                         max_length = 64,           # Pad & truncate all sentences.\n","#                         pad_to_max_length = True,\n","#                         return_attention_mask = True,   # Construct attn. masks.\n","#                         return_tensors = 'pt',     # Return pytorch tensors.\n","#                    )\n","    \n","#     # Add the encoded sentence to the list.    \n","#     input_ids.append(encoded_dict['input_ids'])\n","    \n","#     # And its attention mask (simply differentiates padding from non-padding).\n","#     attention_masks.append(encoded_dict['attention_mask'])\n","\n","# # Convert the lists into tensors.\n","# input_ids = torch.cat(input_ids, dim=0)\n","# attention_masks = torch.cat(attention_masks, dim=0)\n","# labels = torch.tensor(labels)\n","\n","# # Print sentence 0, now as a list of IDs.\n","# print('Original: ', sentences[0])\n","# print('Token IDs:', input_ids[0])"],"metadata":{"id":"jKRItzjxIbbr","executionInfo":{"status":"aborted","timestamp":1658777235696,"user_tz":-180,"elapsed":55,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ExJvOkWaIbbs","executionInfo":{"status":"aborted","timestamp":1658777235697,"user_tz":-180,"elapsed":55,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inference"],"metadata":{"id":"CkkLdJ1iIbbs"}},{"cell_type":"code","source":["# # computing the B3 \n","# test_df[\"B3\"] = test_df.SYSTEM_TRANSCRIPTION.apply(lmr)\n","# # computing the B3 CER\n","# test_df[\"B3_CER\"] = test_df.apply(lambda row: pywer.cer([row.HUMAN_TRANSCRIPTION], [row.B3]), axis=1)\n","# print(\"B3's CER:\", test_df.B3_CER.mean())\n","# # computing B3's CERR\n","# print((test_df.CER - test_df.B3_CER).mean())"],"metadata":{"id":"CTQNZa4TIbbs","executionInfo":{"status":"aborted","timestamp":1658777235698,"user_tz":-180,"elapsed":57,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !nvidia-smi"],"metadata":{"id":"SqCkWC1sIbbs","executionInfo":{"status":"aborted","timestamp":1658777235701,"user_tz":-180,"elapsed":59,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install torchinfo"],"metadata":{"id":"yVrJPYhRIbbs","executionInfo":{"status":"aborted","timestamp":1658777235703,"user_tz":-180,"elapsed":61,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from torchinfo import summary"],"metadata":{"id":"iK4yKkNUIbbt","executionInfo":{"status":"aborted","timestamp":1658777235704,"user_tz":-180,"elapsed":62,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# summary(bert2bert)"],"metadata":{"id":"YBtpHdmmIbbt","executionInfo":{"status":"aborted","timestamp":1658777235712,"user_tz":-180,"elapsed":70,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generating the Prediction File"],"metadata":{"id":"lNgsHQtxfXKO"}},{"cell_type":"code","source":["# using the 1st baseline (B6)\n","submission = pd.DataFrame(zip(test_df.IMAGE_PATH, test_df.B6), columns=[\"ImageID\", \"Transcriptions\"])\n","submission.head()"],"metadata":{"id":"QbZheRAceSlO","executionInfo":{"status":"aborted","timestamp":1658777235714,"user_tz":-180,"elapsed":72,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"sEDADaC5eSh_","executionInfo":{"status":"aborted","timestamp":1658777235720,"user_tz":-180,"elapsed":78,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %aicrowd submission create -c htrec-2022 -f submission.csv"],"metadata":{"id":"n3cbzndHeSeO","executionInfo":{"status":"aborted","timestamp":1658777235722,"user_tz":-180,"elapsed":79,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0xKiV8NbeSae","executionInfo":{"status":"aborted","timestamp":1658777235724,"user_tz":-180,"elapsed":81,"user":{"displayName":"manos papadatos","userId":"10945744099030918704"}}},"execution_count":null,"outputs":[]}]}